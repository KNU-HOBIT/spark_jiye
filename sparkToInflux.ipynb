{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILES IN THIS DIRECTORY\n",
      "['.bashrc', '.bash_logout', '.profile', '.ipython', '.cache', '.npm', '.bash_history', '.local', '.ipynb_checkpoints', 'config.json', '.jupyter', 'jars', '.conda', '.config', '.wget-hsts', 'work']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 06:22:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Spark configuration:\n",
      "spark.app.id = app-20240531062239-0041\n",
      "spark.app.initial.jar.urls = spark://10.42.2.18:39337/jars/spark-token-provider-kafka-0-10_2.12-3.2.4.jar,spark://10.42.2.18:39337/jars/commons-logging-1.1.3.jar,spark://10.42.2.18:39337/jars/commons-pool2-2.6.2.jar,spark://10.42.2.18:39337/jars/spark-sql-kafka-0-10_2.12-3.2.4.jar,spark://10.42.2.18:39337/jars/spark-streaming-kafka-0-10_2.12-3.2.4.jar,spark://10.42.2.18:39337/jars/jsr305-3.0.0.jar,spark://10.42.2.18:39337/jars/htrace-core4-4.1.0-incubating.jar,spark://10.42.2.18:39337/jars/hadoop-client-api-3.3.1.jar,spark://10.42.2.18:39337/jars/hadoop-client-runtime-3.3.1.jar,spark://10.42.2.18:39337/jars/kafka-clients-2.8.1.jar\n",
      "spark.app.name = asdf\n",
      "spark.app.startTime = 1717136558696\n",
      "spark.app.submitTime = 1717136558621\n",
      "spark.cores.max = 32\n",
      "spark.defaul.parallelism = 96\n",
      "spark.driver.bindAddress = 0.0.0.0\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false --illegal-access=permit\n",
      "spark.driver.host = 10.42.2.18\n",
      "spark.driver.memory = 30g\n",
      "spark.driver.port = 39337\n",
      "spark.executor.cores = 16\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false --illegal-access=permit\n",
      "spark.executor.id = driver\n",
      "spark.executor.instances = 3\n",
      "spark.executor.memory = 24G\n",
      "spark.jars = jars/jsr305-3.0.0.jar,jars/commons-logging-1.1.3.jar,jars/commons-pool2-2.6.2.jar,jars/hadoop-client-api-3.3.1.jar,jars/hadoop-client-runtime-3.3.1.jar,jars/htrace-core4-4.1.0-incubating.jar,jars/kafka-clients-2.8.1.jar,jars/spark-token-provider-kafka-0-10_2.12-3.2.4.jar,jars/spark-sql-kafka-0-10_2.12-3.2.4.jar,jars/spark-streaming-kafka-0-10_2.12-3.2.4.jar\n",
      "spark.master = spark://spark-master-service:7077\n",
      "spark.memory.offHeap.enabled = true\n",
      "spark.memory.offHeap.size = 20g\n",
      "spark.network.timeout = 600s\n",
      "spark.rdd.compress = True\n",
      "spark.repl.local.jars = file:///home/jovyan/jars/jsr305-3.0.0.jar,file:///home/jovyan/jars/commons-logging-1.1.3.jar,file:///home/jovyan/jars/commons-pool2-2.6.2.jar,file:///home/jovyan/jars/hadoop-client-api-3.3.1.jar,file:///home/jovyan/jars/hadoop-client-runtime-3.3.1.jar,file:///home/jovyan/jars/htrace-core4-4.1.0-incubating.jar,file:///home/jovyan/jars/kafka-clients-2.8.1.jar,file:///home/jovyan/jars/spark-token-provider-kafka-0-10_2.12-3.2.4.jar,file:///home/jovyan/jars/spark-sql-kafka-0-10_2.12-3.2.4.jar,file:///home/jovyan/jars/spark-streaming-kafka-0-10_2.12-3.2.4.jar\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.sql.shuffle.partitions = 96\n",
      "spark.submit.deployMode = client\n",
      "spark.submit.pyFiles = \n",
      "spark.ui.showConsoleProgress = true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import time as timer\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "from pyspark.sql.functions import col, abs, mean, expr, substring, udf\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "#import lightgbm as lgb\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.driver.extraJavaOptions=-Dio.netty.tryReflectionSetAccessible=true --conf spark.executor.extraJavaOptions=-Dio.netty.tryReflectionSetAccessible=true pyspark-shell'\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"num_date_time\", StringType()),\n",
    "        StructField(\"건물번호\", StringType()),\n",
    "        StructField(\"일시\", StringType()),\n",
    "        StructField(\"기온(C)\", StringType()),\n",
    "        StructField(\"강수량(mm)\", StringType()),\n",
    "        StructField(\"풍속(m/s)\", StringType()),\n",
    "        StructField(\"습도(%)\", StringType()),\n",
    "        StructField(\"일조(hr)\", StringType()),\n",
    "        StructField(\"일사(MJ/m2)\", StringType()),\n",
    "        StructField(\"전력소비량(kWh)\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "schema2 = StructType(\n",
    "    [\n",
    "        StructField(\"num_date_time\", StringType()),\n",
    "        StructField(\"건물번호\", StringType()),\n",
    "        StructField(\"일시\", StringType()),\n",
    "        StructField(\"기온(C)\", StringType()),\n",
    "        StructField(\"강수량(mm)\", StringType()),\n",
    "        StructField(\"풍속(m/s)\", StringType()),\n",
    "        StructField(\"습도(%)\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "schema3 = StructType(\n",
    "    [\n",
    "        StructField(\"건물번호\", StringType()),\n",
    "        StructField(\"건물유형\", StringType()),\n",
    "        StructField(\"연면적(m2)\", StringType()),\n",
    "        StructField(\"냉방면적(m2)\", StringType()),\n",
    "        StructField(\"태양광용량(kW)\", StringType()),\n",
    "        StructField(\"ESS저장용량(kWh)\", StringType()),\n",
    "        StructField(\"PCS용량(kW)\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"FILES IN THIS DIRECTORY\")\n",
    "print(os.listdir(os.getcwd()))\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정\n",
    "\n",
    "#config.json 파일 읽기\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "   config = json.load(f)\n",
    "\n",
    "jar_urls = \",\".join(config[\"KAFKA_JAR_URLS\"])\n",
    "repartition_num = config[\"NUM_EXECUTORS\"] * config[\"EXECUTOR_CORES\"] * 2\n",
    "# SparkSession 생성\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"spark://spark-master-service:7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.host\", \"10.42.2.18\")\n",
    "    .config(\"spark.driver.port\", \"39337\")\n",
    "    .config(\"spark.cores.max\", \"32\")\n",
    "    .config(\"spark.network.timeout\", \"600s\")\n",
    "    .config(\"spark.executor.instances\", config[\"NUM_EXECUTORS\"])\n",
    "    .config(\"spark.executor.cores\", config[\"EXECUTOR_CORES\"])\n",
    "    .config(\"spark.executor.memory\", config[\"EXECUTOR_MEMORY\"])   \n",
    "    .config(\"spark.driver.memory\", \"30g\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\",\"20g\")\n",
    "    \n",
    "    .config(\"spark.defaul.parallelism\", repartition_num)\n",
    "    .config(\"spark.sql.shuffle.partitions\", repartition_num)\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"--illegal-access=permit\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"--illegal-access=permit\")\n",
    "    .config(\"spark.jars\", jar_urls)  # JAR 파일 포함\n",
    "    .appName(\"asdf\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Current Spark configuration:\")\n",
    "for key, value in sorted(sc._conf.getAll(), key=lambda x: x[0]):\n",
    "    print(f\"{key} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- createTime: string (nullable = true)\n",
      " |-- 건물번호: string (nullable = true)\n",
      " |-- 건물유형: string (nullable = true)\n",
      " |-- 연면적(m2): string (nullable = true)\n",
      " |-- 냉방면적(m2): string (nullable = true)\n",
      " |-- 태양광용량(kW): string (nullable = true)\n",
      " |-- ESS저장용량(kWh): string (nullable = true)\n",
      " |-- PCS용량(kW): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그냥 가져오기\n",
    "building_sdf = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"my-cluster-kafka-bootstrap.kafka.svc:9092\")\n",
    "    .option(\"subscribe\", \"building-jy\")\n",
    "    .option(\"kafka.group.id\", \"my_consumer_group\")\n",
    "    .load()\n",
    ")  # 밀리초 단위 에포치 시간endingTimestamp\n",
    "building_sdf = building_sdf.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS STRING)\")\n",
    "building_sdf = building_sdf.withColumnRenamed(\"timestamp\", \"createTime\")\n",
    "building_sdf = building_sdf.withColumn(\"value\", from_json(building_sdf[\"value\"], schema3))\n",
    "\n",
    "\n",
    "for field in schema3.fields:\n",
    "    building_sdf = building_sdf.withColumn(field.name, building_sdf[\"value.\" + field.name])\n",
    "building_sdf = building_sdf.drop(\"value\")\n",
    "# 이거쓰면 df가 repartition_num 수만큼 쪼개져서 병렬처리가능한 상태가 됨.\n",
    "building_sdf = building_sdf.repartition(repartition_num)\n",
    "\n",
    "\n",
    "building_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------+----------+------------+--------------+----------------+-----------+\n",
      "|          createTime|건물번호|      건물유형|연면적(m2)|냉방면적(m2)|태양광용량(kW)|ESS저장용량(kWh)|PCS용량(kW)|\n",
      "+--------------------+--------+--------------+----------+------------+--------------+----------------+-----------+\n",
      "|2024-05-28 13:57:...|       9|      건물기타| 222882.35|    15651.18|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      33|    데이터센터|   28059.0|     20397.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      39|백화점및아울렛|  126835.0|     65596.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      29|        대학교|  199623.0|     77202.0|          25.0|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      27|        대학교|578484.113|   501381.53|          30.0|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      97|  호텔및리조트|  55144.67|     25880.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      54|          상용|  109400.2|    65803.57|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      76|        연구소|  38668.17|    38452.11|         100.0|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      89|      할인마트|   78454.0|     13362.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|       5|      건물기타|  205884.0|    150000.0|          NULL|            2557|       1000|\n",
      "|2024-05-28 13:34:...|      58|          상용|   14897.0|     7071.92|          9.45|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      79|  지식산업센터| 212995.84|   103115.37|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      81|  지식산업센터| 115358.28|    65020.94|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      84|  지식산업센터|  174408.2|    109973.1|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      97|  호텔및리조트|  55144.67|     25880.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      50|          병원|   91967.0|     75836.0|           3.0|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      94|  호텔및리조트|   62156.0|     56060.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      92|      할인마트|   80491.0|     18549.5|         98.28|            NULL|       NULL|\n",
      "|2024-05-28 13:57:...|      87|      할인마트|   65118.0|     34007.0|          NULL|            NULL|       NULL|\n",
      "|2024-05-28 13:34:...|      90|      할인마트|   77917.0|     35676.0|          NULL|            NULL|       NULL|\n",
      "+--------------------+--------+--------------+----------+------------+--------------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "building_sdf = building_sdf.select(\n",
    "    building_sdf[\"createTime\"],\n",
    "    building_sdf[\"건물번호\"],\n",
    "    building_sdf[\"건물유형\"],\n",
    "    building_sdf[\"연면적(m2)\"],\n",
    "    building_sdf[\"냉방면적(m2)\"],\n",
    "    building_sdf[\"태양광용량(kW)\"],\n",
    "    building_sdf[\"ESS저장용량(kWh)\"],\n",
    "    building_sdf[\"PCS용량(kW)\"]\n",
    ") \\\n",
    ".withColumn(\"createTime\", building_sdf[\"createTime\"].cast(StringType())) \\\n",
    ".withColumn(\"건물번호\", building_sdf[\"건물번호\"].cast(IntegerType())) \\\n",
    ".withColumn(\"건물유형\", building_sdf[\"건물유형\"].cast(StringType())) \\\n",
    ".withColumn(\"연면적(m2)\", building_sdf[\"연면적(m2)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"냉방면적(m2)\", building_sdf[\"냉방면적(m2)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"태양광용량(kW)\", building_sdf[\"태양광용량(kW)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"ESS저장용량(kWh)\", building_sdf[\"ESS저장용량(kWh)\"].cast(IntegerType())) \\\n",
    ".withColumn(\"PCS용량(kW)\", building_sdf[\"PCS용량(kW)\"].cast(IntegerType()))\n",
    "\n",
    "building_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- createTime: string (nullable = true)\n",
      " |-- num_date_time: string (nullable = true)\n",
      " |-- 건물번호: string (nullable = true)\n",
      " |-- 일시: string (nullable = true)\n",
      " |-- 기온(C): string (nullable = true)\n",
      " |-- 강수량(mm): string (nullable = true)\n",
      " |-- 풍속(m/s): string (nullable = true)\n",
      " |-- 습도(%): string (nullable = true)\n",
      " |-- 일조(hr): string (nullable = true)\n",
      " |-- 일사(MJ/m2): string (nullable = true)\n",
      " |-- 전력소비량(kWh): string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------+-----------+-------+----------+---------+-------+--------+-----------+---------------+\n",
      "|          createTime| num_date_time|건물번호|       일시|기온(C)|강수량(mm)|풍속(m/s)|습도(%)|일조(hr)|일사(MJ/m2)|전력소비량(kWh)|\n",
      "+--------------------+--------------+--------+-----------+-------+----------+---------+-------+--------+-----------+---------------+\n",
      "|2024-05-28 13:49:...|49_20220818 10|      49|20220818 10|   25.4|      NULL|      0.6|     92|     0.0|        0.0|         3857.4|\n",
      "|2024-05-28 06:46:...|96_20220712 15|      96|20220712 15|   27.3|      NULL|      2.7|     73|     0.6|       2.66|        3394.08|\n",
      "|2024-05-28 06:45:...|74_20220731 21|      74|20220731 21|   26.0|       6.4|      2.3|     94|    NULL|       NULL|        2632.32|\n",
      "|2024-05-28 06:44:...|16_20220715 00|      16|20220715 00|   25.4|      NULL|      1.5|     87|    NULL|       NULL|        1300.32|\n",
      "|2024-05-28 06:45:...|49_20220717 08|      49|20220717 08|   25.1|      NULL|      0.8|     99|     0.0|       0.42|        2988.72|\n",
      "|2024-05-28 06:46:...|80_20220602 01|      80|20220602 01|   17.6|      NULL|      0.8|     69|    NULL|       NULL|         963.36|\n",
      "|2024-05-28 13:49:...|12_20220604 11|      12|20220604 11|   25.4|      NULL|      1.9|     52|     0.8|       2.05|        1542.78|\n",
      "|2024-05-28 13:49:...|12_20220612 22|      12|20220612 22|   21.5|      NULL|      1.8|     83|    NULL|       NULL|        1014.66|\n",
      "|2024-05-28 06:45:...|48_20220804 12|      48|20220804 12|   30.8|      NULL|      1.5|     79|     0.7|        2.8|        2742.12|\n",
      "|2024-05-28 06:44:...|17_20220620 23|      17|20220620 23|   24.2|      NULL|      1.2|     79|    NULL|       NULL|         609.12|\n",
      "|2024-05-28 13:50:...|91_20220611 06|      91|20220611 06|   18.8|      NULL|      1.3|     90|     0.0|       0.08|         429.12|\n",
      "|2024-05-28 06:45:...|35_20220706 17|      35|20220706 17|   33.2|      NULL|      2.1|     62|     0.3|       1.49|         2272.2|\n",
      "|2024-05-28 13:50:...|98_20220627 08|      98|20220627 08|   23.4|      21.3|      1.0|     97|     0.0|       NULL|        1160.64|\n",
      "|2024-05-28 13:50:...|94_20220802 12|      94|20220802 12|   28.1|       0.1|      3.8|     82|     0.0|       0.61|        3006.72|\n",
      "|2024-05-28 13:50:...|72_20220728 06|      72|20220728 06|   23.8|      NULL|      0.3|     99|     0.0|       0.07|        1149.84|\n",
      "|2024-05-28 06:45:...|29_20220726 10|      29|20220726 10|   28.2|      NULL|      2.6|     81|     1.0|       NULL|         2044.5|\n",
      "|2024-05-28 06:46:...|82_20220814 07|      82|20220814 07|   24.5|      NULL|      0.2|     93|     0.0|       0.15|        1243.44|\n",
      "|2024-05-28 06:45:...|68_20220806 02|      68|20220806 02|   27.9|      NULL|      2.1|     85|    NULL|       NULL|         1862.7|\n",
      "|2024-05-28 13:49:...|45_20220626 04|      45|20220626 04|   25.0|      NULL|      3.3|     84|    NULL|       NULL|         2421.6|\n",
      "|2024-05-28 13:50:...|96_20220805 01|      96|20220805 01|   27.5|      NULL|      2.5|     90|    NULL|       NULL|        2293.56|\n",
      "+--------------------+--------------+--------+-----------+-------+----------+---------+-------+--------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# 그냥 가져오기\n",
    "train_sdf = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"my-cluster-kafka-bootstrap.kafka.svc:9092\")\n",
    "    .option(\"subscribe\", \"test-jy\")\n",
    "    .option(\"kafka.group.id\", \"my_consumer_group\")\n",
    "    .load()\n",
    ")  # 밀리초 단위 에포치 시간endingTimestamp\n",
    "\n",
    "train_sdf = train_sdf.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS STRING)\")\n",
    "train_sdf = train_sdf.withColumnRenamed(\"timestamp\", \"createTime\")\n",
    "train_sdf = train_sdf.withColumn(\"value\", from_json(train_sdf[\"value\"], schema))\n",
    "\n",
    "\n",
    "for field in schema.fields:\n",
    "    train_sdf = train_sdf.withColumn(field.name, train_sdf[\"value.\" + field.name])\n",
    "train_sdf = train_sdf.drop(\"value\")\n",
    "# 이거쓰면 df가 repartition_num 수만큼 쪼개져서 병렬처리가능한 상태가 됨.\n",
    "\n",
    "\n",
    "train_sdf = train_sdf.repartition(repartition_num)\n",
    "\n",
    "train_sdf.printSchema()\n",
    "\n",
    "train_sdf = train_sdf.select(\n",
    "    train_sdf[\"createTime\"],\n",
    "    train_sdf[\"num_date_time\"],\n",
    "    train_sdf[\"건물번호\"],\n",
    "    train_sdf[\"일시\"],\n",
    "    train_sdf[\"기온(C)\"],\n",
    "    train_sdf[\"강수량(mm)\"],\n",
    "    train_sdf[\"풍속(m/s)\"],\n",
    "    train_sdf[\"습도(%)\"],\n",
    "    train_sdf[\"일조(hr)\"],\n",
    "    train_sdf[\"일사(MJ/m2)\"],\n",
    "    train_sdf[\"전력소비량(kWh)\"],\n",
    ") \\\n",
    ".withColumn(\"createTime\", train_sdf[\"createTime\"].cast(StringType())) \\\n",
    ".withColumn(\"num_date_time\", train_sdf[\"num_date_time\"].cast(StringType())) \\\n",
    ".withColumn(\"건물번호\", train_sdf[\"건물번호\"].cast(IntegerType())) \\\n",
    ".withColumn(\"일시\", train_sdf[\"일시\"].cast(StringType())) \\\n",
    ".withColumn(\"기온(C)\", train_sdf[\"기온(C)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"강수량(mm)\", train_sdf[\"강수량(mm)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"풍속(m/s)\", train_sdf[\"풍속(m/s)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"습도(%)\", train_sdf[\"습도(%)\"].cast(IntegerType())) \\\n",
    ".withColumn(\"일조(hr)\", train_sdf[\"일조(hr)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"일사(MJ/m2)\", train_sdf[\"일사(MJ/m2)\"].cast(DoubleType())) \\\n",
    ".withColumn(\"전력소비량(kWh)\", train_sdf[\"전력소비량(kWh)\"].cast(DoubleType()))\n",
    "\n",
    "train_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = train_sdf.drop(\"createTime\")\n",
    "building_sdf = building_sdf.drop(\"createTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_x(df):\n",
    "    to_remove_columns = ['num_date_time', '일시', '일조(hr)', '일사(MJ/m2)']\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # 시계열 특성을 학습에 반영하기 위해 일시를 월, 일, 시간으로 나눕니다\n",
    "    df = df.withColumn('month', substring('일시', 5, 2).cast(IntegerType()))\n",
    "    df = df.withColumn('day', substring('일시', 7, 2).cast(IntegerType()))\n",
    "    df = df.withColumn('time', substring('일시', 10, 2).cast(IntegerType()))\n",
    "\n",
    "    df = df.join(building_sdf.select('건물번호', '건물유형', '연면적(m2)'), on='건물번호', how='left')\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    # '건물유형'을 카테고리형 코드로 변환\n",
    "    building_type_indexer = StringIndexer(inputCol='건물유형', outputCol='건물유형_index')\n",
    "    df = building_type_indexer.fit(df).transform(df)\n",
    "    df = df.drop('건물유형').withColumnRenamed('건물유형_index', '건물유형')\n",
    "    \n",
    "    # 불필요한 컬럼 삭제\n",
    "    for c in to_remove_columns:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c)\n",
    "            \n",
    "    df.show(20, truncate=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---------+-------+---------------+-----+---+----+----------+--------+\n",
      "|건물번호|기온(C)|강수량(mm)|풍속(m/s)|습도(%)|전력소비량(kWh)|month|day|time|연면적(m2)|건물유형|\n",
      "+--------+-------+----------+---------+-------+---------------+-----+---+----+----------+--------+\n",
      "|98      |23.2   |0.0       |0.5      |97     |438.3          |7    |26 |3   |53578.62  |10.0    |\n",
      "|67      |23.8   |0.0       |1.6      |95     |864.48         |7    |24 |3   |85244.0   |6.0     |\n",
      "|82      |21.3   |5.6       |4.2      |97     |1555.92        |8    |9  |23  |225651.0  |8.0     |\n",
      "|69      |32.0   |0.0       |1.2      |55     |3925.2         |7    |2  |14  |139928.73 |7.0     |\n",
      "|19      |25.3   |0.9       |3.7      |93     |748.8          |6    |29 |1   |90730.4   |1.0     |\n",
      "|97      |27.8   |0.0       |5.8      |77     |1543.32        |6    |25 |17  |55144.67  |10.0    |\n",
      "|46      |26.2   |0.0       |1.0      |80     |2245.92        |6    |27 |7   |85869.49  |4.0     |\n",
      "|12      |29.5   |0.0       |3.3      |54     |1767.24        |6    |25 |12  |70163.1   |0.0     |\n",
      "|9       |29.6   |0.0       |1.5      |66     |3485.28        |8    |18 |11  |222882.35 |0.0     |\n",
      "|23      |29.9   |0.0       |1.3      |66     |1775.4         |7    |4  |13  |32236.11  |1.0     |\n",
      "|40      |24.3   |0.7       |0.9      |99     |1246.8         |7    |11 |8   |58483.0   |3.0     |\n",
      "|66      |30.3   |0.0       |1.3      |66     |485.64         |8    |21 |16  |105073.0  |6.0     |\n",
      "|19      |26.7   |0.0       |1.5      |93     |749.88         |7    |7  |1   |90730.4   |1.0     |\n",
      "|22      |25.8   |0.0       |1.4      |100    |1056.6         |8    |19 |22  |61375.47  |1.0     |\n",
      "|23      |29.2   |0.0       |2.1      |70     |1694.4         |8    |19 |11  |32236.11  |1.0     |\n",
      "|43      |19.4   |0.0       |1.6      |89     |433.2          |8    |24 |1   |148883.85 |3.0     |\n",
      "|37      |28.2   |0.0       |2.6      |70     |5616.0         |8    |10 |14  |98528.77  |3.0     |\n",
      "|11      |17.4   |0.0       |1.7      |72     |1517.52        |6    |14 |21  |45956.56  |0.0     |\n",
      "|48      |26.9   |0.0       |2.9      |99     |1766.52        |8    |20 |6   |66729.0   |4.0     |\n",
      "|13      |24.6   |0.0       |2.2      |92     |2385.12        |7    |28 |6   |5578.4    |0.0     |\n",
      "+--------+-------+----------+---------+-------+---------------+-----+---+----+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "before_split_sdf = preprocess_x(train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "before_split_sdf = before_split_sdf.select(\n",
    "    before_split_sdf[\"건물번호\"],\n",
    "    before_split_sdf[\"기온(C)\"],\n",
    "    before_split_sdf[\"강수량(mm)\"],\n",
    "    before_split_sdf[\"풍속(m/s)\"],\n",
    "    before_split_sdf[\"습도(%)\"],\n",
    "    before_split_sdf[\"전력소비량(kWh)\"],\n",
    "    before_split_sdf[\"month\"],\n",
    "    before_split_sdf[\"day\"],\n",
    "    before_split_sdf[\"time\"],\n",
    "    before_split_sdf[\"연면적(m2)\"],\n",
    "    before_split_sdf[\"건물유형\"],\n",
    "    \n",
    ") \\\n",
    ".withColumn(\"건물번호\", before_split_sdf[\"건물번호\"].cast(StringType())) \\\n",
    ".withColumn(\"기온(C)\", before_split_sdf[\"기온(C)\"].cast(StringType())) \\\n",
    ".withColumn(\"강수량(mm)\", before_split_sdf[\"강수량(mm)\"].cast(StringType())) \\\n",
    ".withColumn(\"풍속(m/s)\", before_split_sdf[\"풍속(m/s)\"].cast(StringType())) \\\n",
    ".withColumn(\"습도(%)\", before_split_sdf[\"습도(%)\"].cast(StringType())) \\\n",
    ".withColumn(\"전력소비량(kWh)\", before_split_sdf[\"전력소비량(kWh)\"].cast(StringType())) \\\n",
    ".withColumn(\"month\", before_split_sdf[\"month\"].cast(StringType())) \\\n",
    ".withColumn(\"day\", before_split_sdf[\"day\"].cast(StringType())) \\\n",
    ".withColumn(\"time\", before_split_sdf[\"time\"].cast(StringType())) \\\n",
    ".withColumn(\"연면적(m2)\", before_split_sdf[\"연면적(m2)\"].cast(StringType())) \\\n",
    ".withColumn(\"건물유형\", before_split_sdf[\"건물유형\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# 스파크 데이터프레임을 판다스 데이터프레임으로 변환\n",
    "pandas_df = before_split_sdf.toPandas()\n",
    "\n",
    "pandas_df = pandas_df.where(pd.notnull(pandas_df), None)\n",
    "\n",
    "pandas_df = pandas_df.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '기온(C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '전력소비량(kWh)': 'power_consumption',\n",
    "    '연면적(m2)': 'total_area',\n",
    "    '건물유형': 'building_type',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_number</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>power_consumption</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>total_area</th>\n",
       "      <th>building_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>97</td>\n",
       "      <td>438.3</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>53578.62</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>23.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>95</td>\n",
       "      <td>864.48</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>85244.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>21.3</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>97</td>\n",
       "      <td>1555.92</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>225651.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55</td>\n",
       "      <td>3925.2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>139928.73</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>93</td>\n",
       "      <td>748.8</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>90730.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  building_number temperature rainfall windspeed humidity power_consumption  \\\n",
       "0              98        23.2      0.0       0.5       97             438.3   \n",
       "1              67        23.8      0.0       1.6       95            864.48   \n",
       "2              82        21.3      5.6       4.2       97           1555.92   \n",
       "3              69        32.0      0.0       1.2       55            3925.2   \n",
       "4              19        25.3      0.9       3.7       93             748.8   \n",
       "\n",
       "  month day time total_area building_type  \n",
       "0     7  26    3   53578.62          10.0  \n",
       "1     7  24    3    85244.0           6.0  \n",
       "2     8   9   23   225651.0           8.0  \n",
       "3     7   2   14  139928.73           7.0  \n",
       "4     6  29    1    90730.4           1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfluxDB 클라이언트 설정\n",
    "bucket = \"electric\"\n",
    "org = \"influxdata\"\n",
    "token = \"LQcRh6vq3wXU_QWpYFEhmk2IRZxgIn04ByEYHWW6WZ9xhNwyQB-2K6K_faA-CzvWlreXT0EES1Xz10STwAu0hQ==\"\n",
    "url = \"http://155.230.34.52:32145/\"\n",
    "\n",
    "client = InfluxDBClient(url=url, token=token, org=org)\n",
    "write_api = client.write_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 판다스 데이터프레임을 InfluxDB에 적재\n",
    "for index, row in pandas_df.iterrows():\n",
    "    \n",
    "    year = 2022\n",
    "    # month, day, time 컬럼에서 값 추출\n",
    "    month = int(row['month']) if row['month'] is not None else 1\n",
    "    day = int(row['day']) if row['day'] is not None else 1\n",
    "    hour = int(row['time']) if row['time'] is not None else 0\n",
    "    \n",
    "    # datetime 객체로 변환\n",
    "    dt_obj = datetime(year, month, day, hour)\n",
    "    \n",
    "    # Unix 타임스탬프 나노초 단위로 변환\n",
    "    timestamp_ns = int(time.mktime(dt_obj.timetuple()) * 1e9)\n",
    "    \n",
    "    # JSON 형태의 필드 값 생성\n",
    "    fields_value = {\n",
    "        \"building_number\": row['building_number'],\n",
    "        \"temperature\": float(row['temperature']) if row['temperature'] is not None else None,\n",
    "        \"rainfall\": float(row['rainfall']) if row['rainfall'] is not None else None,\n",
    "        \"windspeed\": float(row['windspeed']) if row['windspeed'] is not None else None,\n",
    "        \"humidity\": float(row['humidity']) if row['humidity'] is not None else None,\n",
    "        \"power_consumption\": float(row['power_consumption']) if row['power_consumption'] is not None else None,\n",
    "        \"month\": float(row['month']) if row['month'] is not None else None,\n",
    "        \"day\": float(row['day']) if row['day'] is not None else None,\n",
    "        \"time\": float(row['time']) if row['time'] is not None else None,\n",
    "        \"total_area\": float(row['total_area']) if row['total_area'] is not None else None,\n",
    "        \"building_type\": float(row['building_type']) if row['building_type'] is not None else None\n",
    "    }\n",
    "    \n",
    "    \n",
    "    point = Point(\"electric_dataset\") \\\n",
    "        .tag(\"building_number\", row['building_number']) \\\n",
    "        .field(\"value\", json.dumps(fields_value)) \\\n",
    "        .time(timestamp_ns, WritePrecision.NS)\n",
    "    \n",
    "    write_api.write(bucket=bucket, org=org, record=point)\n",
    "\n",
    "write_api.__del__()\n",
    "client.__del__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"markdown","source":["### Label Encoding 수행\n* StringIndexer 객체를 이용하여 Label Encoding 적용\n* StringIndexer 객체 생성 시 변환될 컬럼명과 변환 후 컬럼명을 입력 받음.\n* StringIndexer 객체의 fit()메소드 호출 시 DataFrame 입력하면 StringInxerModel이 반환됨.\n* 반환된 StringInxerModel 객체의 transform() 메소드 호출시 DataFrame 입력하면 Label Encoding 적용된 outputCol이 추가된 DataFrame반환."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bb49124-e423-4ada-8967-07f0e070beb8"}}},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n    [\"id\", \"category\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d6fe57f-3dfa-4202-9b23-55ed809b95bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------+\n| id|category|\n+---+--------+\n|  0|       a|\n|  1|       b|\n|  2|       c|\n|  3|       a|\n|  4|       a|\n|  5|       c|\n+---+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------+\n| id|category|\n+---+--------+\n|  0|       a|\n|  1|       b|\n|  2|       c|\n|  3|       a|\n|  4|       a|\n|  5|       c|\n+---+--------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\n# StringIndexer 클래스의 생성 인자로 DataFrame에서 Label 변환이 될 컬럼명인 inputCol, 그리고 변환 결과 컬럼명인 outputCol 필요)\nindexer = StringIndexer(inputCol='category', outputCol='category_index')\n\n# StringIndexer는 fit() 수행시 DataFrame을 입력 받고, StringIndexerModel 객체를 반환함. \nindexer_model = indexer.fit(df)\nprint(indexer_model)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"329e7ff8-333c-45dc-a495-4b0f0c474f64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"StringIndexerModel: uid=StringIndexer_0ec8942b1277, handleInvalid=error\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["StringIndexerModel: uid=StringIndexer_0ec8942b1277, handleInvalid=error\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# StringIndexerModel에 transform()을 적용하여 outputCol로 지정된 컬럼명으로 Label Encoding 적용한 DataFrame 생성 반환. \nindexed_df = indexer_model.transform(df)\nindexed_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e58f1004-a860-41b8-b6af-4543a6a16dcf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------+--------------+\n| id|category|category_index|\n+---+--------+--------------+\n|  0|       a|           0.0|\n|  1|       b|           2.0|\n|  2|       c|           1.0|\n|  3|       a|           0.0|\n|  4|       a|           0.0|\n|  5|       c|           1.0|\n+---+--------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------+--------------+\n| id|category|category_index|\n+---+--------+--------------+\n|  0|       a|           0.0|\n|  1|       b|           2.0|\n|  2|       c|           1.0|\n|  3|       a|           0.0|\n|  4|       a|           0.0|\n|  5|       c|           1.0|\n+---+--------+--------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### IndexToString 클래스를 이용하여 Label Encoding된 값을 원본 값으로 원복 할 수 있음."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5fa6b47-60de-477e-985f-36c738d6b12e"}}},{"cell_type":"code","source":["from pyspark.ml.feature import IndexToString\n\nconverter = IndexToString(inputCol='category_index', outputCol='original_category')\nconverted = converter.transform(indexed_df)\nconverted.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4577f56d-c226-43e3-a272-c229475d37e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------+--------------+-----------------+\n| id|category|category_index|original_category|\n+---+--------+--------------+-----------------+\n|  0|       a|           0.0|                a|\n|  1|       b|           2.0|                b|\n|  2|       c|           1.0|                c|\n|  3|       a|           0.0|                a|\n|  4|       a|           0.0|                a|\n|  5|       c|           1.0|                c|\n+---+--------+--------------+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------+--------------+-----------------+\n| id|category|category_index|original_category|\n+---+--------+--------------+-----------------+\n|  0|       a|           0.0|                a|\n|  1|       b|           2.0|                b|\n|  2|       c|           1.0|                c|\n|  3|       a|           0.0|                a|\n|  4|       a|           0.0|                a|\n|  5|       c|           1.0|                c|\n+---+--------+--------------+-----------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### 여러개의 컬럼을 Label Encoding 수행. \n* StringIndexer 객체 생성 시 inputCols에 리스트로 변환될 컬럼들을 입력하고, outputCols에 새롭게 변환된 컬럼명을 입력"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2d3cc48-6bcb-40cb-acb2-c48b629901cd"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\ndf = spark.createDataFrame(\n    [(0, \"a\", \"A\"), (1, \"b\", \"A\"), (2, \"c\", \"K\"), (3, \"a\", \"D\"), (4, \"a\", \"C\"), (5, \"c\", \"B\")],\n    [\"id\", \"category1\", \"category2\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e9c174f-6563-403b-bf0a-9e17ea6629cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+---------+\n| id|category1|category2|\n+---+---------+---------+\n|  0|        a|        A|\n|  1|        b|        A|\n|  2|        c|        K|\n|  3|        a|        D|\n|  4|        a|        C|\n|  5|        c|        B|\n+---+---------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+---------+\n| id|category1|category2|\n+---+---------+---------+\n|  0|        a|        A|\n|  1|        b|        A|\n|  2|        c|        K|\n|  3|        a|        D|\n|  4|        a|        C|\n|  5|        c|        B|\n+---+---------+---------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["indexer = StringIndexer(inputCols=[\"category1\", \"category2\"], outputCols=[\"label_encoded1\", \"label_encoded2\"])\nindexed_model = indexer.fit(df)\nindexed_df = indexed_model.transform(df)\nindexed_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"662aa03f-eb9a-4f2a-aac4-d0a39c7711bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+---------+--------------+--------------+\n| id|category1|category2|label_encoded1|label_encoded2|\n+---+---------+---------+--------------+--------------+\n|  0|        a|        A|           0.0|           0.0|\n|  1|        b|        A|           2.0|           0.0|\n|  2|        c|        K|           1.0|           4.0|\n|  3|        a|        D|           0.0|           3.0|\n|  4|        a|        C|           0.0|           2.0|\n|  5|        c|        B|           1.0|           1.0|\n+---+---------+---------+--------------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+---------+--------------+--------------+\n| id|category1|category2|label_encoded1|label_encoded2|\n+---+---------+---------+--------------+--------------+\n|  0|        a|        A|           0.0|           0.0|\n|  1|        b|        A|           2.0|           0.0|\n|  2|        c|        K|           1.0|           4.0|\n|  3|        a|        D|           0.0|           3.0|\n|  4|        a|        C|           0.0|           2.0|\n|  5|        c|        B|           1.0|           1.0|\n+---+---------+---------+--------------+--------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### One Hot Encoding 적용\n* OneHotEncoder 클래스를 이용하여 변환\n* OneHotEncoder될 컬럼은 반드시 숫자형으로 변환되어 있어야 함. 따라서 OneHotEncoder를 String 컬럼에 적용 시에는 Label Encoding을 먼저 적용 후에 변환해야 함."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d60c33cd-09b0-444c-b582-75c820edbc4e"}}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n\ndf = spark.createDataFrame([\n    (0.0, 1.0),\n    (1.0, 0.0),\n    (2.0, 1.0),\n    (0.0, 2.0),\n    (0.0, 1.0),\n    (2.0, 0.0)\n], [\"categoryIndex1\", \"categoryIndex2\"])\n\n# dropLast는 마지막 인자를 제외할지를 나타냄 default는 True. \n# 5개의 카테고리(0, 1, 2, 3, 4)가 있을 경우 2는 [0.0, 0.0, 1.0, 0.0] 로 매핑. 4는 [0.0, 0.0, 0.0, 0.0]로 매핑. \nencoder = OneHotEncoder(dropLast=True, inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n                        outputCols=[\"onehot_encoded1\", \"onehot_encoded2\"])\nencoded_model = encoder.fit(df)\n# OneHotEncoder는 sparse vector 형태로 onehot encoding 적용. \nencoded_df = encoded_model.transform(df)\n#encoded_df = encoded_model.fit(df).transform(df) # scitkit의 fit_transform()과 같이 좀 더 축약된 코드\n\nprint(encoded_df.show())\ndisplay(encoded_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e0c924c-ce39-4873-b107-5bdd011014ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+--------------+---------------+---------------+\n|categoryIndex1|categoryIndex2|onehot_encoded1|onehot_encoded2|\n+--------------+--------------+---------------+---------------+\n|           0.0|           1.0|  (3,[0],[1.0])|  (3,[1],[1.0])|\n|           1.0|           0.0|  (3,[1],[1.0])|  (3,[0],[1.0])|\n|           2.0|           1.0|  (3,[2],[1.0])|  (3,[1],[1.0])|\n|           0.0|           2.0|  (3,[0],[1.0])|  (3,[2],[1.0])|\n|           0.0|           1.0|  (3,[0],[1.0])|  (3,[1],[1.0])|\n|           2.0|           0.0|  (3,[2],[1.0])|  (3,[0],[1.0])|\n+--------------+--------------+---------------+---------------+\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+--------------+---------------+---------------+\n|categoryIndex1|categoryIndex2|onehot_encoded1|onehot_encoded2|\n+--------------+--------------+---------------+---------------+\n|           0.0|           1.0|  (3,[0],[1.0])|  (3,[1],[1.0])|\n|           1.0|           0.0|  (3,[1],[1.0])|  (3,[0],[1.0])|\n|           2.0|           1.0|  (3,[2],[1.0])|  (3,[1],[1.0])|\n|           0.0|           2.0|  (3,[0],[1.0])|  (3,[2],[1.0])|\n|           0.0|           1.0|  (3,[0],[1.0])|  (3,[1],[1.0])|\n|           2.0|           0.0|  (3,[2],[1.0])|  (3,[0],[1.0])|\n+--------------+--------------+---------------+---------------+\n\nNone\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0.0,1.0,{"vectorType":"sparse","length":3,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":3,"indices":[1],"values":[1.0]}],[1.0,0.0,{"vectorType":"sparse","length":3,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":3,"indices":[0],"values":[1.0]}],[2.0,1.0,{"vectorType":"sparse","length":3,"indices":[2],"values":[1.0]},{"vectorType":"sparse","length":3,"indices":[1],"values":[1.0]}],[0.0,2.0,{"vectorType":"sparse","length":3,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":3,"indices":[2],"values":[1.0]}],[0.0,1.0,{"vectorType":"sparse","length":3,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":3,"indices":[1],"values":[1.0]}],[2.0,0.0,{"vectorType":"sparse","length":3,"indices":[2],"values":[1.0]},{"vectorType":"sparse","length":3,"indices":[0],"values":[1.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"categoryIndex1","type":"\"double\"","metadata":"{}"},{"name":"categoryIndex2","type":"\"double\"","metadata":"{}"},{"name":"onehot_encoded1","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"0\"},{\"idx\":1,\"name\":\"1\"},{\"idx\":2,\"name\":\"2\"}]},\"num_attrs\":3}}"},{"name":"onehot_encoded2","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"0\"},{\"idx\":1,\"name\":\"1\"},{\"idx\":2,\"name\":\"2\"}]},\"num_attrs\":3}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>categoryIndex1</th><th>categoryIndex2</th><th>onehot_encoded1</th><th>onehot_encoded2</th></tr></thead><tbody><tr><td>0.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(1), values -> List(1.0))</td></tr><tr><td>1.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(2), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(1), values -> List(1.0))</td></tr><tr><td>0.0</td><td>2.0</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(2), values -> List(1.0))</td></tr><tr><td>0.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(1), values -> List(1.0))</td></tr><tr><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(2), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(0, \"a\", \"A\"), (1, \"b\", \"A\"), (2, \"c\", \"K\"), (3, \"a\", \"D\"), (4, \"a\", \"C\"), (5, \"c\", \"B\")],\n    [\"id\", \"category1\", \"category2\"])\nprint(df.show())\n\nencoder = OneHotEncoder(inputCols=[\"category1\", \"category2\"],\n                        outputCols=[\"onehot_encoded1\", \"onehot_encoded2\"])\n                        \n# 아래 코드는 string값을 One Hot Encoding 적용 시도 하였기에 오류 발생. \nencoded_model = encoder.fit(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"828f7013-e6ce-4408-91e7-ec956ac91fdd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+---------+\n| id|category1|category2|\n+---+---------+---------+\n|  0|        a|        A|\n|  1|        b|        A|\n|  2|        c|        K|\n|  3|        a|        D|\n|  4|        a|        C|\n|  5|        c|        B|\n+---+---------+---------+\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+---------+\n| id|category1|category2|\n+---+---------+---------+\n|  0|        a|        A|\n|  1|        b|        A|\n|  2|        c|        K|\n|  3|        a|        D|\n|  4|        a|        C|\n|  5|        c|        B|\n+---+---------+---------+\n\nNone\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-1916978023792376>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# 아래 코드는 string값을 One Hot Encoding 적용 시도 하였기에 오류 발생.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mencoded_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py\u001B[0m in \u001B[0;36mpatched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m             \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m                 \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36msafe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    523\u001B[0m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    524\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 525\u001B[0;31m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    526\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    527\u001B[0m                     \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"succeeded\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mpatch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 242\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    243\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mException\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m                 \u001B[0;31m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mpatched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    923\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_log\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    924\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0m_AUTOLOGGING_METRICS_MANAGER\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable_log_post_training_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 925\u001B[0;31m                     \u001B[0mfit_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_mlflow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    926\u001B[0m                 \u001B[0;31m# In some cases the `fit_result` may be an iterator of spark models.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    927\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mshould_log_post_training_metrics\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfit_result\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mfit_mlflow\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[0;31m# Note that featurizers will be autologged when they're fit as part of a Pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    901\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_get_fully_qualified_class_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pyspark.ml.feature.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 902\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    903\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m             \u001B[0;31m# skip the case params is a list or tuple, this case it will call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mcall_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m                                 \u001B[0mdisable_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreroute_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    480\u001B[0m                             ):\n\u001B[0;32m--> 481\u001B[0;31m                                 \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mog_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mog_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    482\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    483\u001B[0m                             try_log_autologging_event(\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \"\"\"\n\u001B[1;32m    331\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column category1 must be of type numeric but was actually of type string.","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Column category1 must be of type numeric but was actually of type string.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-1916978023792376>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# 아래 코드는 string값을 One Hot Encoding 적용 시도 하였기에 오류 발생.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mencoded_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py\u001B[0m in \u001B[0;36mpatched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m             \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m                 \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36msafe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    523\u001B[0m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    524\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 525\u001B[0;31m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    526\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    527\u001B[0m                     \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"succeeded\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mpatch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 242\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    243\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mException\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m                 \u001B[0;31m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mpatched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    923\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_log\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    924\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0m_AUTOLOGGING_METRICS_MANAGER\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable_log_post_training_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 925\u001B[0;31m                     \u001B[0mfit_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_mlflow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    926\u001B[0m                 \u001B[0;31m# In some cases the `fit_result` may be an iterator of spark models.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    927\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mshould_log_post_training_metrics\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfit_result\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mfit_mlflow\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[0;31m# Note that featurizers will be autologged when they're fit as part of a Pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    901\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_get_fully_qualified_class_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pyspark.ml.feature.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 902\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    903\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m             \u001B[0;31m# skip the case params is a list or tuple, this case it will call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mcall_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m                                 \u001B[0mdisable_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreroute_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    480\u001B[0m                             ):\n\u001B[0;32m--> 481\u001B[0;31m                                 \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mog_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mog_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    482\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    483\u001B[0m                             try_log_autologging_event(\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \"\"\"\n\u001B[1;32m    331\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column category1 must be of type numeric but was actually of type string."]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(0, \"a\", \"A\"), (1, \"b\", \"A\"), (2, \"c\", \"K\"), (3, \"a\", \"D\"), (4, \"a\", \"C\"), (5, \"c\", \"B\")],\n    [\"id\", \"category1\", \"category2\"])\n\n# StringIndexer를 이용하여 label encoding 적용. \nlabel_encoder = StringIndexer(inputCols=[\"category1\", \"category2\"], outputCols=[\"label_encoded1\", \"label_encoded2\"])\nlabel_encoded_df = label_encoder.fit(df).transform(df)\n\n# 앞에서 숫자로 변환된 label encoding 컬럼들을 One Hot encoding 적용. \nonehot_encoder = OneHotEncoder(inputCols=[\"label_encoded1\", \"label_encoded2\"],\n                        outputCols=[\"onehot_encoded1\", \"onehot_encoded2\"])\n                        \n# 앞에서 Label encoding 변환된 DataFrame을 이용해서 One Hot encoding 적용해야함\nonehot_encoded_df = onehot_encoder.fit(label_encoded_df).transform(label_encoded_df)\n\ndisplay(onehot_encoded_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f40dfd16-dc8f-429d-a0cc-91a3b7b11759"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"a","A",0.0,0.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[1,"b","A",2.0,0.0,{"vectorType":"sparse","length":2,"indices":[],"values":[]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[2,"c","K",1.0,4.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[],"values":[]}],[3,"a","D",0.0,3.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[3],"values":[1.0]}],[4,"a","C",0.0,2.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[2],"values":[1.0]}],[5,"c","B",1.0,1.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[1],"values":[1.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"category1","type":"\"string\"","metadata":"{}"},{"name":"category2","type":"\"string\"","metadata":"{}"},{"name":"label_encoded1","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"label_encoded1\"}}"},{"name":"label_encoded2","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"A\",\"B\",\"C\",\"D\",\"K\"],\"type\":\"nominal\",\"name\":\"label_encoded2\"}}"},{"name":"onehot_encoded1","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"a\"},{\"idx\":1,\"name\":\"c\"}]},\"num_attrs\":2}}"},{"name":"onehot_encoded2","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"A\"},{\"idx\":1,\"name\":\"B\"},{\"idx\":2,\"name\":\"C\"},{\"idx\":3,\"name\":\"D\"}]},\"num_attrs\":4}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>category1</th><th>category2</th><th>label_encoded1</th><th>label_encoded2</th><th>onehot_encoded1</th><th>onehot_encoded2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>A</td><td>0.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>1</td><td>b</td><td>A</td><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2</td><td>c</td><td>K</td><td>1.0</td><td>4.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(), values -> List())</td></tr><tr><td>3</td><td>a</td><td>D</td><td>0.0</td><td>3.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(3), values -> List(1.0))</td></tr><tr><td>4</td><td>a</td><td>C</td><td>0.0</td><td>2.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(2), values -> List(1.0))</td></tr><tr><td>5</td><td>c</td><td>B</td><td>1.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(1), values -> List(1.0))</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Pipeline을 이용하여 OneHot Encoding 적용 \n* StringIndexer 객체와 OneHotEncoder 객체를 각각 stage로 Pipeline에 등록하여 encoding 변환."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b752cb45-7d20-4919-8ba2-1fa37249a7fc"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# Stage로 사용될 StringIndexer 객체와 OneHotEncoder 객체 생성. \nstage_1 = StringIndexer(inputCols=['category1', 'category2'], outputCols=['label_encoded1', 'label_encoded2'])\nstage_2 = OneHotEncoder(inputCols=['label_encoded1', 'label_encoded2'], outputCols=['onehot_encoded1', 'onehot_encoded2'])\n\n# stage로 StringIndexer객체와 OneHotEncoder 객체 등록하여 Pipeline 객체 생성. \npipeline = Pipeline(stages=[stage_1, stage_2])\n\n# pipeline.fit(df) 수행하여 PipelineModel 생성하고 PipelineModel의 transfrom(df) 호출하여 최종 변환. \npipeline_model = pipeline.fit(df)\nonehot_encoded_df = pipeline_model.transform(df)\n#onehot_encoded_df = pipeline.fit(df).transform(df)\n\ndisplay(onehot_encoded_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28b94b72-543d-4c0b-a5c1-e0e677f5e66b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"a","A",0.0,0.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[1,"b","A",2.0,0.0,{"vectorType":"sparse","length":2,"indices":[],"values":[]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[2,"c","K",1.0,4.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[],"values":[]}],[3,"a","D",0.0,3.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[3],"values":[1.0]}],[4,"a","C",0.0,2.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[2],"values":[1.0]}],[5,"c","B",1.0,1.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[1],"values":[1.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"category1","type":"\"string\"","metadata":"{}"},{"name":"category2","type":"\"string\"","metadata":"{}"},{"name":"label_encoded1","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"label_encoded1\"}}"},{"name":"label_encoded2","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"A\",\"B\",\"C\",\"D\",\"K\"],\"type\":\"nominal\",\"name\":\"label_encoded2\"}}"},{"name":"onehot_encoded1","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"a\"},{\"idx\":1,\"name\":\"c\"}]},\"num_attrs\":2}}"},{"name":"onehot_encoded2","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"A\"},{\"idx\":1,\"name\":\"B\"},{\"idx\":2,\"name\":\"C\"},{\"idx\":3,\"name\":\"D\"}]},\"num_attrs\":4}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>category1</th><th>category2</th><th>label_encoded1</th><th>label_encoded2</th><th>onehot_encoded1</th><th>onehot_encoded2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>A</td><td>0.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>1</td><td>b</td><td>A</td><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2</td><td>c</td><td>K</td><td>1.0</td><td>4.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(), values -> List())</td></tr><tr><td>3</td><td>a</td><td>D</td><td>0.0</td><td>3.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(3), values -> List(1.0))</td></tr><tr><td>4</td><td>a</td><td>C</td><td>0.0</td><td>2.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(2), values -> List(1.0))</td></tr><tr><td>5</td><td>c</td><td>B</td><td>1.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(1), values -> List(1.0))</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(pipeline_model.stages)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"816c208a-0e28-45d9-a1ec-08a03643332d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[StringIndexerModel: uid=StringIndexer_34f4af80678e, handleInvalid=error, numInputCols=2, numOutputCols=2, OneHotEncoderModel: uid=OneHotEncoder_9409ee9b63ee, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[StringIndexerModel: uid=StringIndexer_34f4af80678e, handleInvalid=error, numInputCols=2, numOutputCols=2, OneHotEncoderModel: uid=OneHotEncoder_9409ee9b63ee, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Scaling의 적용\n* Standard 스케일링은 StandardScaler 클래스로, Min Max 스케일링은 MinMaxClass를 이용하여 적용. \n* 주의할 사용한 Scaling은 일반 컬럼형(숫자형)이 아니라 vector형에만 적용이 가능함. 이는 Spark ML이 통계 전용의 기능을 제공하기 보다는 ML에 주로 특화 되었기 때문\n* 때문에 단일 컬럼에 Scaling을 적용할 때도 반드시 VectorAssembler로 변환 후에 적용해야 함"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f566ef0-4d96-4df7-9852-f46a15195989"}}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\nimport numpy as np\nimport pandas as pd\n\n# iris 데이터 세트 로딩하고  iris 데이터 세트를 numpy에서 pandas DataFrame으로 변환 \niris = load_iris()\niris_data = iris.data\niris_label = iris.target\n\niris_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\niris_pdf = pd.DataFrame(iris_data, columns=iris_columns)\niris_pdf['label'] = iris_label\n\n# pandas DataFrame을 spark DataFrame으로 변환\niris_sdf = spark.createDataFrame(iris_pdf)\n\ndisplay(iris_sdf.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d1d7319-a910-4ba5-aaf4-81019e95ef33"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\n\n# number type 단일 컬럼에 StandardScaler를 적용하면 오류 발생. Vector 형으로 해당 컬럼을 변경해야 함. \nstandard_scaler = StandardScaler(inputCol='sepal_length', outputCol='scaled_sepal_length')\nstandard_scaler_model = standard_scaler.fit(iris_sdf)\nstandard_scaled_df = standard_scaler_model.transform(iris_sdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50a96e37-143d-4ab5-aa57-023352ecccf9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\n\n# VectorAssembler는 반드시 생성자로 inputCols를 list 형으로 받아야 함. inputCol은 안됨.  \n# vec_assembler = VectorAssembler(inputCol=sepal_length, outputCol='sepal_length_vector')는 오류 발생. \nvec_assembler = VectorAssembler(inputCols=['sepal_length'], outputCol='sepal_length_vector')\n\n# VectorAssembler는 fit()이 없음. \niris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n\ndisplay(iris_sdf_vectorized.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc2dbb04-110e-4d1e-b10d-34cf7a84a303"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# vector화된 컬럼에 대해서 StandardScaler 적용 \nstandard_scaler = StandardScaler(inputCol='sepal_length_vector', outputCol='standard_scaled_vector_01')\nstandard_scaler_model = standard_scaler.fit(iris_sdf_vectorized)\nstandard_scaled_df = standard_scaler_model.transform(iris_sdf_vectorized)\ndisplay(standard_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9bb1082-fa68-4884-8277-49a9cc1ca22f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["standard_scaler = StandardScaler(inputCol='sepal_length_vector', outputCol='standard_scaled_vector_02', withMean=True, withStd=True)\nstandard_scaler_model = standard_scaler.fit(iris_sdf_vectorized)\nstandard_scaled_df = standard_scaler_model.transform(iris_sdf_vectorized)\ndisplay(standard_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c427503-5ebc-4777-9195-c938e9534ca7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 전체 컬럼에 Standard Scaler 적용. \nvec_assembler = VectorAssembler(inputCols=iris_columns, outputCol='features')\nstandard_scaler = StandardScaler(inputCol='features', outputCol='standard_scaled_features', withMean=True, withStd=True)\n\niris_sdf_vectorized = vec_assembler.transform(iris_sdf)\nstandard_scaled_df = standard_scaler.fit(iris_sdf_vectorized).transform(iris_sdf_vectorized)\n\nstandard_scaled_df.limit(10).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4deb838f-0994-42b9-8d2c-b350bbd46289"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Pipeline을 이용하여 Standard Scaling  변환\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [vec_assembler, standard_scaler])\nstandard_scaled_df = pipeline.fit(iris_sdf).transform(iris_sdf)\n\nstandard_scaled_df.limit(10).show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b71b189c-23a5-44a4-ad71-1e1ff0a9f823"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### MinMax 스케일링 변환"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb5509cc-6099-4485-bd66-ebe2575e70d7"}}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\n\n# 전체 feature 컬럼에 minmax scaler 적용\nvec_assembler = VectorAssembler(inputCols=iris_columns, outputCol='features')\niris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n\nminmax_scaler = MinMaxScaler(inputCol='features', outputCol='minmax_scaled_features')\nminmax_scaled_df = minmax_scaler.fit(iris_sdf_vectorized).transform(iris_sdf_vectorized)\n\ndisplay(minmax_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"320b0cf3-717c-4e38-93ed-2f3e4d446173"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [vec_assembler, minmax_scaler])\nminmax_scaled_df = pipeline.fit(iris_sdf).transform(iris_sdf)\n\ndisplay(minmax_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a970ad9a-42ab-466b-aebd-8a87ca3c9716"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54b53069-281e-42fb-9b06-b1a5e0e6d120"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"spark_encoding_scaling","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1916978023792364}},"nbformat":4,"nbformat_minor":0}

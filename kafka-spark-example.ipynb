{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import time as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 'value' 열의 JSON 문자열을 파싱하여 각각의 열로 만들기\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType()),\n",
    "        StructField(\"blk_no\", StringType()),\n",
    "        StructField(\"press3\", IntegerType()),\n",
    "        StructField(\"calc_press2\", DoubleType()),\n",
    "        StructField(\"press4\", IntegerType()),\n",
    "        StructField(\"calc_press1\", DoubleType()),\n",
    "        StructField(\"calc_press4\", DoubleType()),\n",
    "        StructField(\"calc_press3\", DoubleType()),\n",
    "        StructField(\"bf_gps_lon\", DoubleType()),\n",
    "        StructField(\"gps_lat\", DoubleType()),\n",
    "        StructField(\"speed\", DoubleType()),\n",
    "        StructField(\"in_dt\", StringType()),\n",
    "        StructField(\"move_time\", DoubleType()),\n",
    "        StructField(\"dvc_id\", StringType()),\n",
    "        StructField(\"dsme_lat\", DoubleType()),\n",
    "        StructField(\"press1\", IntegerType()),\n",
    "        StructField(\"press2\", IntegerType()),\n",
    "        StructField(\"work_status\", IntegerType()),\n",
    "        StructField(\"timestamp\", StringType()),\n",
    "        StructField(\"is_adjust\", StringType()),\n",
    "        StructField(\"move_distance\", IntegerType()),\n",
    "        StructField(\"weight\", DoubleType()),\n",
    "        StructField(\"dsme_lon\", DoubleType()),\n",
    "        StructField(\"in_user\", StringType()),\n",
    "        StructField(\"eqp_id\", IntegerType()),\n",
    "        StructField(\"blk_get_seq_id\", IntegerType()),\n",
    "        StructField(\"lot_no\", StringType()),\n",
    "        StructField(\"proj_no\", StringType()),\n",
    "        StructField(\"gps_lon\", DoubleType()),\n",
    "        StructField(\"seq_id\", LongType()),\n",
    "        StructField(\"bf_gps_lat\", DoubleType()),\n",
    "        StructField(\"blk_dvc_id\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"FILES IN THIS DIRECTORY\")\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json 파일 읽기\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "jar_urls = \",\".join(config[\"KAFKA_JAR_URLS\"])\n",
    "repartition_num = config[\"NUM_EXECUTORS\"] * config[\"EXECUTOR_CORES\"] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 생성\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"spark://spark-master-service:7077\")\n",
    "    .config(\"spark.driver.host\", \"10.42.2.119\")\n",
    "    .config(\"spark.driver.port\", \"39337\")\n",
    "    .config(\"spark.executor.instances\", config[\"NUM_EXECUTORS\"])\n",
    "    .config(\"spark.executor.cores\", config[\"EXECUTOR_CORES\"])\n",
    "    .config(\"spark.executor.memory\", config[\"EXECUTOR_MEMORY\"])\n",
    "    .config(\"spark.defaul.parallelism\", repartition_num)\n",
    "    .config(\"spark.sql.shuffle.partitions\", repartition_num)\n",
    "    .config(\"spark.jars\", jar_urls)  # JAR 파일 포함\n",
    "    .appName(\"asdf\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Spark configuration:\")\n",
    "for key, value in sorted(sc._conf.getAll(), key=lambda x: x[0]):\n",
    "    print(f\"{key} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 가져오기\n",
    "df = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"my-cluster-kafka-bootstrap.kafka.svc:9092\")\n",
    "    .option(\"subscribe\", \"my-topic\")\n",
    "    .load()\n",
    ")  # 밀리초 단위 에포치 시간endingTimestamp\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS STRING)\")\n",
    "df = df.withColumnRenamed(\"timestamp\", \"createTime\")\n",
    "df = df.withColumn(\"value\", from_json(df[\"value\"], schema))\n",
    "for field in schema.fields:\n",
    "    df = df.withColumn(field.name, df[\"value.\" + field.name])\n",
    "df = df.drop(\"value\")\n",
    "# 이거쓰면 df가 repartition_num 수만큼 쪼개져서 병렬처리가능한 상태가 됨.\n",
    "df = df.repartition(repartition_num)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타임스탬프를 밀리초 단위로 에포치(epoch)하는 것은 Unix epoch 시간 형식을 사용하는 것을 의미합니다. Unix epoch 시간은 1970년 1월 1일 00:00:00 UTC를 기준으로 경과된 시간을 초 또는 밀리초 단위로 나타낸 것입니다.\n",
    "\n",
    "Unix 에포치 시간\n",
    "Unix 에포치 시간은 특정 시점의 시간을 나타내는 일반적인 방식으로, 아래와 같이 두 가지 방법으로 나타낼 수 있습니다:\n",
    "\n",
    "초 단위로 표현:\n",
    "예: 1633052800 (이는 2021년 10월 1일 00:00:00 UTC에 해당합니다)\n",
    "밀리초 단위로 표현:\n",
    "예: 1633052800000 (이는 2021년 10월 1일 00:00:00 UTC에 해당합니다)\n",
    "밀리초 단위는 초 단위보다 정확한 표현으로, 초 단위의 값에 1000을 곱하여 밀리초 단위로 변환할 수 있습니다.\n",
    "\n",
    "startingTimestamp에 밀리초 단위 Unix 타임스탬프 사용\n",
    "startingTimestamp 옵션은 Unix epoch 시간을 밀리초 단위로 받습니다. 예를 들어, 2021년 10월 1일 00:00:00 UTC 이후에 생성된 메시지를 읽으려면 startingTimestamp를 1633052800000으로 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def to_milliseconds_epoch(timestamp_str):\n",
    "    # 주어진 문자열을 datetime 객체로 변환\n",
    "    dt = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    # 에포치 시간을 구한 뒤 밀리초 단위로 변환\n",
    "    milliseconds_epoch = int(dt.timestamp() * 1000)\n",
    "    return milliseconds_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "result = to_milliseconds_epoch(\"2024-05-05 05:50:20.652\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메세지 생성 시간을 Unix 밀리초 단위로 에포치한 값으로 변환하여, 시간 범위로 데이터 가져오기!!\n",
    "# 메세지 생성이 2024-05-05 05:50:20.652 이때 된 메세지의 createTime을 startingTimestamp로 설정\n",
    "# endingTimestamp는 대충 임의로 10_000_000 밀리초정도 더 높게 설정.\n",
    "# startingOffsetsByTimestampStrategy 설정을 통해서, No offset matched from request of topic 오류 해결.\n",
    "df = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"my-cluster-kafka-bootstrap.kafka.svc:9092\")\n",
    "    .option(\"subscribe\", \"my-topic\")\n",
    "    .option(\"startingOffsetsByTimestampStrategy\", \"latest\")\n",
    "    .option(\"startingTimestamp\", \"1714888220652\")\n",
    "    .option(\"endingTimestamp\", \"1714898220652\")\n",
    "    .load()\n",
    ")  # 밀리초 단위 에포치 시간endingTimestamp\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS STRING)\")\n",
    "df = df.withColumnRenamed(\"timestamp\", \"createTime\")\n",
    "df = df.withColumn(\"value\", from_json(df[\"value\"], schema))\n",
    "for field in schema.fields:\n",
    "    df = df.withColumn(field.name, df[\"value.\" + field.name])\n",
    "df = df.drop(\"value\")\n",
    "# 이거쓰면 df가 repartition_num 수만큼 쪼개져서 병렬처리가능한 상태가 됨.\n",
    "df = df.repartition(repartition_num)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 종료\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
